<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本文设计了一款AST级和token级代码完成的统一多任务学习模型">
<meta property="og:type" content="article">
<meta property="og:title" content="AST级和token级代码完成的统一多任务学习模型">
<meta property="og:url" content="http://example.com/2022/11/09/AST%E7%BA%A7%E5%92%8Ctoken%E7%BA%A7%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%88%90%E7%9A%84%E7%BB%9F%E4%B8%80%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="A_Niucw的小屋">
<meta property="og:description" content="本文设计了一款AST级和token级代码完成的统一多任务学习模型">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-11-09T08:44:41.455Z">
<meta property="article:modified_time" content="2022-11-09T08:44:31.186Z">
<meta property="article:author" content="A_Niucw">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2022/11/09/AST%E7%BA%A7%E5%92%8Ctoken%E7%BA%A7%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%88%90%E7%9A%84%E7%BB%9F%E4%B8%80%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>AST级和token级代码完成的统一多任务学习模型 | A_Niucw的小屋</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="A_Niucw的小屋" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">A_Niucw的小屋</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/11/09/AST%E7%BA%A7%E5%92%8Ctoken%E7%BA%A7%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%88%90%E7%9A%84%E7%BB%9F%E4%B8%80%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="A_Niucw">
      <meta itemprop="description" content="静下来搞技术">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A_Niucw的小屋">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AST级和token级代码完成的统一多任务学习模型
        </h1>

        <div class="post-meta">
	
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-11-09 16:44:41 / 修改时间：16:44:31" itemprop="dateCreated datePublished" datetime="2022-11-09T16:44:41+08:00">2022-11-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%88%90/" itemprop="url" rel="index"><span itemprop="name">代码完成</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文设计了一款AST级和token级代码完成的统一多任务学习模型</p>
<span id="more"></span>

<h1 id="AST级和token级代码完成的统一多任务学习模型"><a href="#AST级和token级代码完成的统一多任务学习模型" class="headerlink" title="AST级和token级代码完成的统一多任务学习模型"></a>AST级和token级代码完成的统一多任务学习模型</h1><h2 id="Abstract："><a href="#Abstract：" class="headerlink" title="Abstract："></a>Abstract：</h2><p>代码完成（代码表示）：应用在继承开发环境中进行预测之后的代码</p>
<p>依靠统计语言模型的循环神经网络通过大范围项目学习可以增强代码完成的能力</p>
<p>现在的方法不考虑诸如静态类型信息之类的语法约束而只是根据上下文的语法去预测AST或者Token的值，并只是简单的把这个过程视为一个生成任务</p>
<p>现有的基于递归神经网络的语言模型不足以对长期依赖进行建模</p>
<p>本篇文章依赖于AST-level和code-level的代码完成的方式创建了统一多任务的学习解决了上述的限制（无法考虑语法约束）。</p>
<p>引入了多任务学习框架来预测–&gt;AST- node的值和类型建模</p>
<p>引入基于自注意结构的网络作为基础语言模型–&gt;捕捉输入程序中的长期依赖性</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>statistical language models-&gt;依靠于已存在代码的重复性观察和预测N-gram被广泛使用</p>
<p>深度学习发展—-&gt;基于递归神经网络的模型</p>
<p>上述将代码写成抽象语法树（AST）节点序列，通过一部分代码预测最可能出现的下一个代码，存在局限。</p>
<p>1）分级结构信息未在项目表示中得到充分利用（token-based model简单的化成token序列而无法考虑语法和代码结构，AST- based model 遍历树被展平所以分层级别被忽视，树的结构信息未被充分利用）</p>
<p>2）循环神经网络（RNN）无法对long-term semantic（语义） dependency进行很好的捕捉</p>
<p>    当前的代码完成研究创建的长期短期存储器（LSTM）无法对源代码中的长期依赖进行建模</p>
<p>    长期依赖 (Long-Term Dependency) 是指使用神经网络优化算法时随着结构的逐步加深，模型会不可避免地丧失学习到之前信息的能力，让优化变得极其困难）</p>
<p>3）源代码元素的类型信息使用不足，类型和值之间的关系被忽略。</p>
<p>    而AST层面的代码完成研究没有充分考虑他们之间的这些关系。</p>
<p>    令牌级的代码完成技术和代码建模技术没有考虑类型信息</p>
<p>本文目的</p>
<p>    提出统一多任务的代码完成学习框架（讨论AST级和令牌级的代码完成）</p>
<p>        AST级代码完成将代码视为抽象语法树，并基于从给定部分AST获得的信息进行预测。–&gt;更好的利用程序的语法和结构信息</p>
<p>        令牌级代码完成将代码视为源令牌序列，并基于从现有token序列获得的信息进行预测。—&gt;token序列&#x3D;&#x3D;键入的自然序列</p>
<p>        该框架允许我们执行一系列设计决策，这些决策可以帮助我们在完成系统的期望属性之间进行良好的权衡。</p>
<p>四个组件</p>
<p>    1）代码元素编码器，将AST节点或源代码令牌编码为分布式矢量表示。</p>
<p>        包括字编码器和子字编码器</p>
<p>        其中子字编码器可以更好地捕获由几个子字组成的标识符的语义。</p>
<p>    2）上下文代码编码器</p>
<p>        将上下文编码为分布式矢量表示</p>
<p>        使用RNN- based（基于循环神经网络）Transformer-based 编码器，比较他们的结果</p>
<p>    3）Path2root编码器，对从预测节点到根节点的路径进行编码。</p>
<p>    4）代码元素预测器</p>
<p>        对先前编码器的输出采用多任务学习产生代码完成结果</p>
<p>比较了代码学习的两项任务（类型和属性）的的两种方式</p>
<p>    类型优先（先预测类型，辅助预测值）</p>
<p>    联合预测</p>
<p>AST-level code completion</p>
<p>    为了弥合顺序节点序列和AST的层次结构之间的差距，提取了从预测节点到根节点的路径，这表明了预测节点的层次结构。</p>
<p>    然后使用Path2root编码器将路径信息建模为上下文程序的表示。</p>
<p>    Code Element Encoder使用两种不同的编码方式对节点进行编码</p>
<p>    Contextual Code Encoder使用Transformer-XL network编码上下文</p>
<p>    ASTs有类型和值两种属性</p>
<p>    联合预测和类型有限两种预测方式</p>
<p>token-level code completion</p>
<p>    代码元素编码器采用两种不同的token编码器：word encoder and subword encoder 编码每个token</p>
<p>    Contextual Code Encoder使用Transformer-XL network编码上下文</p>
<p>    创建多任务学习模型预测类型和值</p>
<p>为了评估提出的模型的性能，在AST级和令牌级代码完成的真实数据集上进行了实验。</p>
<p>    AST：Java、python、JavaScript</p>
<p>    token：Java、typescript</p>
<p>研究的扩展方向：</p>
<p>    Unified Multi-Task learning based neural Language Model for code completion called UMTLM并提出了应用token和AST的普遍框架</p>
<p>    探讨了对类型和值学习的两种方式。联合、类型优先</p>
<p>    论了当采用不同的令牌编码器和上下文代码编码器时，性能如何不同。对于令牌编码器，我们考虑字编码器和子字编码器；对于上下文代码编码器，我们考虑基于RNN的编码器和基于Transformer的编码器。</p>
<p>    添加评估指标：包括标识符预测准确性（用于AST级代码完成）和类型预测准确性（对于令牌级代码完成）</p>
<p>contributions</p>
<p>    创建了一种基于神经语言的统一的多任务的代码学习方式，类型和值之间的关系可以提供代码完成过程中的语法语义约束</p>
<p>    将分层结构信息建模到程序的表示中。</p>
<p>    采用Transformer XL网络作为语言模型，以捕获上下文标记之间的长距离依赖关系和语义关系。</p>
<p>    实验验证最佳性能</p>
<h2 id="Motivating-Example"><a href="#Motivating-Example" class="headerlink" title="Motivating Example"></a>Motivating Example</h2><p>AST节点的层次结构</p>
<p>    我们使用“Type[Value]”表示每个节点。从预测节点到根节点的路径表明了预测节点的层次结构</p>
<p>类型和值的关系</p>
<p>    JAVA IDE的访问，依赖于类型的代码完成是可用的</p>
<p>    Python、JS IDEs等动态语言，基于type的代码完成是不可用的</p>
<p>    现如今基于源代码建模技术的静态语言的代码完成研究不考虑两者之间的关系</p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>统计语言模型</p>
<p>    将每个代码块进行标记为st进行评分p(s) &#x3D; p(s1)p(s2 | s1)p(s3 | s1,s2), …,p(st | s1,s2, …,st−1)计算语言</p>
<p>explicit language models（显示语言模型）   增加了对程序上下文的限制</p>
<p>    N-gram models</p>
<p>        令牌的概率仅取决于N− 1个最新令牌 p(st | s1,s2, …,st−1) &#x3D; p(st | st−n+1, …,st−1)</p>
<p>        模型已被证明能够有效地捕获源代码中的重复规则</p>
<p>        只能对顺序依赖性进行建模</p>
<p>    Dependency models</p>
<p>        依赖模型还可以捕获语法或语义依赖</p>
<p>Implicit Language Models（隐式语言模型）</p>
<p>    神经网络对源代码进行建模</p>
<p>    代码模式由优化的高维实值向量隐式表示。使用训练语料库上的梯度下降算法来估计网络的参数。</p>
<p>    Recurrent neural networks（循环神经网络）</p>
<p>        LSTM用于源代码的建模</p>
<p>    Self-attentional neural networks</p>
<p>        递归神经网络梯度消失和路径爆炸—-&gt;在长距离单词对之间添加直接连接的注意机制</p>
<p>        Transformer是一种完全基于注意机制的架构</p>
<p>        提出multi-headed self-attention mechanism来代替重复层，并且它可以减少顺序计算并捕获更长范围的依赖性。</p>
<p>        为了解决Transformer网络在语言建模的设置中受到固定长度上下文的限制—&gt;Transformer XL将递归概念引入深度自注意网络—&gt;对代码长期依赖性进行建模</p>
<p>Multi-task Learning（概念没看懂） MTL</p>
<p>    多任务学习是一种跨相关任务进行知识转移的方法</p>
<p>    深度神经网络的多任务学习的方法：隐藏层的硬参数共享或软参数共享</p>
<h2 id="Proposed-Model（UMTLM模型）"><a href="#Proposed-Model（UMTLM模型）" class="headerlink" title="Proposed Model（UMTLM模型）"></a>Proposed Model（UMTLM模型）</h2><p>总体架构</p>
<p>    首先将源代码先被加工成AST或token</p>
<p>    模型都给出了可能的下一个代码元素（源代码标记或AST节点）的列表，以及从训练语料库中估计的概率。</p>
<p>    Code Element Encoder</p>
<p>        将代码元素（AST节点或源代码标记）ti编码为分布式向量表示xt的神经网络。</p>
<p>            AST级连接类型和值向量</p>
<p>            token级使用word encoder和subword encoder</p>
<p>    Contextual Code Encoder</p>
<p>        将程序上下文x1:t编码为分布式向量表示ht的神经网络。</p>
<p>        为了获取长期依赖，使用Transformer-XL network作为Contextual Code Encoder，实验的时候用RNN进行测试</p>
<p>    Path2root Encoder</p>
<p>        一种神经网络，将AST路径p1:mt编码为向量表示Pmt。</p>
<p>        仅用于AST级代码完成，以弥合顺序节点序列和AST分层结构之间的差距</p>
<p>    Code Element Predictor</p>
<p>        获取先前编码器输出并生成代码完成结果的组件。使用MTL（多任务学习）预测下一个令牌</p>
<p>        使用联合预测和先类型预测后通过类型结果预测进行值预测</p>
<p>    输入序列被馈送到代码元素编码器以获得输入向量x。然后上下文代码编码器将输入向量x编码为分布式向量表示h。 Path2root编码器编码以产生路径向量表示P,最后，上下文代码向量表示h和路径向量P被连接并馈入代码元素预测器，用于预测下一个节点的类型（BinOp）和值（Empty）</p>
<p>Code Element Encoder</p>
<p>    将代码元素（AST节点或源代码标记）ti编码为分布式向量表示xt的神经网络。</p>
<p>    AST级代码完成使用AST来表示程序，然后将其遍历到节点序列。</p>
<p>    token级代码完成将代码转化成令牌序列，静态分析或人工注释，可以获得令牌序列中标识符的类型信息。</p>
<p>        JAVA使用aiXcoder工具提取信息</p>
<p>        typescript应用Hellendorn等人（2018）中的方法来提取标识符的类型注释。</p>
<p>    Word Encoder</p>
<p>        word encoder学习固定词汇表Vt中每个标记或AST节点的值的维度为D的Embedding（文本表示的一类方法）。这需要学习并存储具有｜Vt|×D参数的嵌入矩阵。令牌进行查找的方式：</p>
<p>            Eword(t) &#x3D; EmbeddingLookUp(t,Vt )”&#x2F;&#x2F;查找embedding矩阵t的D维行</p>
<p>        词汇表的大小是一个需要调整的超级参数,较小的词汇表减少了内存需求，但代价是无法表示许多标记，从而导致建议不太准确。</p>
<p>    Subword encoder</p>
<p>        程序编程的单词通常由子单词组成</p>
<p>        子字编码器使用Camelcase和下划线命名约定对每个标记或AST节点的值进行标记化，子字被规范化为小写。</p>
<p>        使用大小为｜Vs|×D的子标记嵌入矩阵来获得每个子标记（eset、emaximum、etime）的Embedding向量，其中Vs是子单词“词汇表”，比完整标记嵌入矩阵小得多</p>
<p>            Esubword &#x3D;⊕ （ts ∈split (t )） EmbeddingLookUp(ts ,Vs)&#x2F;&#x2F;split（）是一个对其输入进行子标记并返回一组子标记的函数，⊕ 是一个聚合运算符，它“概括”了子单词中单个单词的含义。</p>
<p>        作用：可以将长字和不频繁出现的拆分为频繁出现的子字</p>
<p>        优势：可以承担更小的embedding矩阵，帮助理解很少出现的token的含义</p>
<p>    AST Type encoder</p>
<p>        将类型属性编码为AST节点的分布式向量。类型编码器直接学习固定词汇表Vtype中每个类型的维度为Dtype的embedding。这需要学习并存储具有|Vtype|×|Dtype|参数的嵌入矩阵</p>
<p>            Etype(type) &#x3D; EmbeddingLookUp(type,Vtype)&#x2F;&#x2F;返回与类型对应的嵌入矩阵的Dtype维行</p>
<p>        节点的类型的数量固定，远小于节点的值，因此不存在未知类型，可以提供更小的embdedding矩阵</p>
<p>    将它们连接为节点xi的最终表示形式xi&#x3D;[Ti；Vi]，Ti表示类型，Vi表示值</p>
<p>Contextual Code Encoder</p>
<p>    上下文代码：点之前的token&#x2F;节点作为上下文代码</p>
<p>    上下文编码器将完成位置相关的上下文信息编码为表示向量ht</p>
<p>        ht &#x3D; Ecxt (x1,x2, …,xt )</p>
<p>    RNN-based Context Encoder</p>
<p>        用于对输入序列的长范围（long-range dependency）依赖进行建模。(在一定范围内)</p>
<p>        神经网络中的隐藏状态存储了先前时间步中输入的信息，并定期更新：</p>
<p>            hi &#x3D; RNN(hi−1,xi−1)</p>
<p>    Transformer-based Context Encoder</p>
<p>        基于multi-headed自注意机制，可以减少顺序计算并捕获较长范围的依赖性性。</p>
<p>        Transformer XL被提议在Transformer架构中引入递归机制，这使得Transformer网络能够对非常长期的依赖性（long-term dependency）进行建模。</p>
<p>        Transformer XL架构重新使用前面的代码而不是从头开始计算获得输入代码的隐藏状态，创建了重复连接，重复使用的隐藏状态可以用作当前段的内存，这使得信息能够通过重复连接传播。该模型可以捕获非常长期的依赖性</p>
<p>        段sτ的第n层隐藏状态计算公式</p>
<p>        与标准Transformer相比，关键区别在于密钥knτ+1和值vnτ+1是基于扩展上下文h̃n−1τ+1，因此hn−1τ+1从上一段缓存。</p>
<p>        由multi-headed自注意机制和位置全连接前馈网络组成。此外，为了在重用状态时保持位置信息的一致性，采用了相对位置emdbedding</p>
<p>Path2root Encoder</p>
<p>    提取从预测节点到根节点的路径，即p1t，p2t，…，pmt</p>
<p>    使用了基于双向LSTM的Path2root编码器，该编码器对路径中的节点进行编码以生成路径向量。两个方向的隐藏向量：</p>
<p>        Pt &#x3D;[ht(-&gt;)m; ht()&lt;-m]&#x2F;&#x2F;获得每个时间步长的最终路径向量Pt</p>
<p>Code Element Predictor</p>
<p>    联合预测类型和值或首先预测类型，然后利用类型预测结果来辅助值预测。</p>
<p>    联合预测</p>
<p>        基于相同的隐藏向量联合预测下一个令牌的类型和值。</p>
<p>        上下文代码编码器htn的输出和路径向量Pt（用于AST级代码完成）连接以计算输出向量Ot。</p>
<p>        softmax函数来生成输出的概率分布</p>
<p>            Ot &#x3D; tanh(W o(htn; Pt ))</p>
<p>            Y type t &#x3D; softmax(W type y Ot + btype)</p>
<p>            Y value t &#x3D; softmax(W value y Ot + bvalue）</p>
<p>    类型优先预测</p>
<p>        首先基于输出向量Ot预测下一个令牌的类型。然后利用预测的类型向量Etype t来辅助值预测</p>
<p>            类型预测：将上下文代码编码器htn的输出和路径向量Pt（用于AST级代码完成）连接起来，以计算类型预测的输出向量O type t。然后我们使用softmax函数来产生类型预测输出Y类型t的概率分布</p>
<p>            值预测：使用令牌编码器来计算预测类型Etype t的向量表示，将预测类型Etype t的向量、上下文代码编码器htn的输出和路径向量Pt（用于AST级代码完成）连接起来，以计算值Ovalue t的输出向量。然后将输出向量馈入输出softmax层，以计算Y值t的输出矢量</p>
<p>                O type t &#x3D; tanh(W type o (htn; Pt ))</p>
<p>                Y type t &#x3D; softmax(W type y Otype t + btype)</p>
<p>                Etype t &#x3D; Etoken(Y type t ,Vtype)</p>
<p>                O value t &#x3D; tanh(W value o (htn; Pt ; Etype))</p>
<p>                Y value t &#x3D; softmax(W value y Ovalue t + bvalue)</p>
<p>Training</p>
<p>    为了在我们的多任务学习框架中学习类型和价值预测任务，我们采用任务特定损失的加权和作为最终损失：</p>
<p>        loss &#x3D; N ∑ (k&#x3D;1) αk × lossk&#x2F;&#x2F;其中N是任务的数量。αk是第k个任务的损失权重，αk≥ 0, ∑N k&#x3D;1αk&#x3D;1。</p>
<h2 id="Experiment-and-Analysis"><a href="#Experiment-and-Analysis" class="headerlink" title="Experiment and Analysis"></a>Experiment and Analysis</h2><p>数据集和Vocabulary</p>
<p>    AST数据的测试</p>
<p>        Python和Javascript数据集</p>
<p>            从GitHub存储库中收集的，方法是删除重复文件，删除项目分叉，只保留AST中最多有30000个节点的解析程序。每个数据集包含100000个培训项目和50000个测试项目。提供了源代码文件及其相应的AST。</p>
<p>            Python中用于解析ast的解析器是PythonStandardLibrary中的ast模块。</p>
<p>            JavaScript中使用的解析器是Acorn4。</p>
<p>        Java数据集</p>
<p>            Java数据集从Github收集。</p>
<p>            使用javalang转化成ASTs</p>
<p>        对于所有数据集，每个程序都以其AST格式表示，并且AST以预先排序的深度优先遍历方式序列化，以生成AST节点序列。</p>
<p>        节点类型值由三部分组成（类型名、是否有子级、是否有正确的兄弟节点）。&lt;—-为了展开的AST节点能够还原回源代码，添加了子节点和兄弟节点，节类型的数量增加</p>
<p>        然后，生成用于训练和测试的查询，每个AST节点一个，方法是从序列中删除节点和右侧的所有节点，然后尝试预测节点。</p>
<p>        AST Vocabulary</p>
<p>            选择了50000个最频繁的值来构建所有三个数据集的价值词汇表。AST Vocabulary表以外的值，使用UNK来表示。对于Python，Java和JavaScript分别为11%、13%和7%</p>
<p>    Token数据的测试</p>
<p>        Java and TypeScript。Java数据集与AST数据相同</p>
<p>        数据集选择GitHub上stars比较多的项目</p>
<p>        对于JAVA程序，通过静态分析提取标识符的类型信息</p>
<p>        对于TypeScript程序，我们应用Hellendorn等人（2018）中的方法来提取标识符的类型注释。然后我们过滤程序，以确保每个TypeScript文件中至少有10%的类型注释是用户定义的类型。</p>
<p>        将项目按8:1:1的比例分成训练&#x2F;验证&#x2F;测试集</p>
<p>    Token Vocabulary</p>
<p>        每个训练集中选择K（50000）个最频繁的令牌来构建令牌词汇表,Java和TypeScript测试集的值词汇的UNK率分别为10%、5%和9%。</p>
<p>Metrics(评估方法)</p>
<p>    用准确性来评估模型的性能</p>
<p>    模型为给定上下文的下一个令牌&#x2F;AST节点提供了一个有序的建议列表。我们计算下一个令牌&#x2F;AST节点的值和类型的前1精度，即正确建议出现在预测列表的第一个中的次数。</p>
<p>    标点符号，如运算符、大括号等都是代码完成的目标</p>
<p>    在训练和评估过程中，数据语料库中不同数据类型的比例对模型的训练过程有很大影响。超过2&#x2F;3的完成目标（令牌）不是标识符。因此预测这些目标的训练样本多于预测标识符。因此模型将在2&#x2F;3的非标识符令牌上表现得更好</p>
<p>    然而，在实践中，只有与标识符相关的补全。为了提高标识符的完成性能，我们使用标识符的静态类型信息，并且还将标识符预测准确度（ID准确度）作为度量标识符完成性能的度量。</p>
<p>    使用标准化精度改进来衡量“改进空间”</p>
<p>实验设置</p>
<p>研究问题和结果</p>
<p>    与最先进的模型相比，我们提出的方法在AST级别的代码完成方面表现如何？</p>
<p>        与指针混合网络（Pointer Mixture Network）进行比较（基于注意力和指针生成器网络的代码完成模型），与基于深度神经网络的语言模型进行比较（vanilla LSTM和Transformer XL网络）都要更优秀</p>
<p>        大多数OoV（out-of-vocabulary）令牌仍然无法在PMN中正确预测。模型采用了一个强大的主干语言模型，并引入了MTL（多任务学习）和path2root编码器，它们在in-vocabulary-tokens方面，尤其是在标识符预测方面可以优于其他的模型。</p>
<p>        应用Wilcoxon秩和检验（WRST）来检验模型在基线上的改进是否具有统计学意义，并且所有的p值都小于1e-5，这表明了显著的改进。</p>
<p>        使用Cliff’s Delta（Macbeth等人，2011）来测量效果大小，并且这些值是不可忽略的。</p>
<p>        结论：</p>
<p>            与普通LSTM或Transformer XL模型相比，UMTLM与相应的上下文编码器（即UMTLM（RNN）和UMTLM）可以实现更好的性能，这表明所提出的组件对于提高AST级代码完成的性能是有效的。</p>
<p>    提出的方法在令牌级代码完成方面表现如何？</p>
<p>        结论：</p>
<p>            模型在Java和TypeScript数据集上都大大优于PMN，尤其是在标识符完成方面。</p>
<p>            TypeScript数据集的总体改进小于Java，尤其是在标识符完成方面。TypeScript中的标识符比例（9.74%）小于Java（21.04%）</p>
<p>            比较BPE NLM：当评估完成各种令牌时，BPE NLM的性能不如标识符完成。我们的模型在完成所有类型的token方面大大优于BPE NLM。</p>
<p>            与普通LSTM或Transformer XL模型相比，具有相应上下文编码器的UMTLM可以实现更好的性能，这进一步证明了所提出的组件对于提高令牌级代码完成的性能是有效的。</p>
<p>    框架中每个组件在AST级和令牌级代码完成方面的有效性是什么？</p>
<p>        通过消融研究，以检查最佳模型使用组件的效果，多任务学习机制、子字编码器和Path2root编码器。我们在没有这些组件的情况下进行实验。</p>
<p>        从Transformer XL架构中移除递归机制进行实验。移除重复机制之后与指针混合网络中LSTM的展开长度（输入序列的长度）相同。</p>
<p>        结论：</p>
<p>            完整模型中移除重复机制时，精度会下降很多，甚至低于普通Transformer XL网络。</p>
<p>            循环机制使模型能够缓存先前上下文信息的内存，从而可以捕获程序中的长程依赖关系。</p>
<p>            移除子字编码器时，所有数据集的精度也会下降。</p>
<p>            组件对于提高AST代码完成性能是必须的，MTL对改进的贡献更大。</p>
<p>            捕获长期依赖对令牌级代码完成非常重要。</p>
<p>    Computational Cost Per-Suggestion（单个节点的时间消耗）</p>
<p>        计算了我们提出的模型（基于RNN和基于Transformer）和基线模型预测单个建议所需的平均时间。</p>
<p>        总的来说，我们的模型在1毫秒内做出预测，这使它们符合实时代码完成系统的要求。1ms左右</p>
<h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>    Learning Process Analysis</p>
<p>        类型损失表格的原因：</p>
<p>            通过利用AST的分层结构信息和相关任务的训练信号中包含的信息，我们提出的模型可以从程序中提取更准确和通用的特征，从而可以获得更好的性能</p>
<p>            采用Transformer XL架构对程序中的长期依赖进行建模，有助于我们的模型从上下文中获取更多信息</p>
<p>            多任务学习通过任务之间的知识共享提供了一种有效的正则化方法，从而可以通过减少训练和测试损失之间的差异来提高模型的性能，这在一定程度上防止了模型的过度拟合。</p>
<p>    Training Cost Analysis</p>
<p>        评估改进的成本，统计了参数的数量，并记录了最佳模型和PMN的训练时间</p>
<p>        在PMN中，他们采用LSTM作为语言模型，其中大多数递归计算是在隐藏状态的更新过程中执行的。</p>
<p>        UMTLM中Transformer XL被用作语言模型。在Transformer XL中，每个段的每个输入的表示都是根据自关注层来计算的，并且递归只发生在段之间。因此，它允许更多的并行化，并且需要更少的训练时间。</p>
<p>        模型中添加Path2root编码器是对模型结构的改进。它增加了模型的复杂性，从而增加了训练时间。</p>
<p>    权重对任务特定损失的影响</p>
<p>        我们使用任务特定损失的加权和作为最终损失。模型的性能与任务损失之间的权重选择有关。当对任务的损失给予更多的权重时，该任务的准确性将提高。（对应的准确性）</p>
<p>    MTL框架中不同学习机制的影响</p>
<p>        不同学习方式在这两种完成任务中的表现不同。对于AST级别的代码完成，预测两个任务联合执行比类型优先执行更好。对于token级完成，结果是相反的，其中类型优先的方法实现了更好的性能。</p>
<p>        原因在于AST节点上的类型和值之间的相关性比源代码令牌弱</p>
<p>    定性分析</p>
<p>        困难类型预测</p>
<p>            循环、if语句结构、异常处理语句</p>
<p>            模型在所有这些类型中都大大优于PMN。此外，在我们的模型中，预测每个令牌的准确度的方差远小于PMN</p>
<p>            结果表明，我们的模型可以发现程序的结构，并在结构预测方面实现了优异的泛化性能。</p>
<p>        实例代码完成分析</p>
<p>            路径的影响</p>
<p>                在第一个示例中，目标预测名称是函数init的参数，其对应节点的类型是NameParam。从它到根节点的路径暗示了预测是函数参数的信息，因此它可以帮助我们的模型对节点类型进行正确预测。</p>
<p>                基线模型只能从顺序上下文中学习无法产生正确的预测</p>
<p>                在第四个例子，路径无法提供准确的信息，因为函数体中存在许多可能的子项。</p>
<p>            MTL的影响</p>
<p>                第二个例子，模型可以意识到节点的值是先前上下文中已使用的值。因此，它可以从上下文中识别值。对于基线模型，如果没有辅助任务的帮助，它可能无法实现预测是变量访问操作。</p>
<p>    对有效性的威胁</p>
<p>        对外部效度的威胁</p>
<p>            能否进行推广与我们使用的数据集的质量和结果的可推广性有关。</p>
<p>        对内部效度的威胁</p>
<p>            包括每个任务损失之间权重的影响，即αk。</p>
<p>            对内部有效性的另一个威胁与基线方法实施中的错误有关</p>
<p>        对概念效度的威胁</p>
<p>            与我们的评估措施的适用性相关</p>
<p>            使用准确度，准确度是代码完成的经典评估指标，并已用于许多先前的代码完成工作</p>
<p>            令牌级和AST级的实验设计并不完全相同，包括输入格式和模型组件。因此，即使在同一数据集中，AST级和令牌级代码完成的结果也不能严格地相互比较。</p>
<p>                输入格式不同</p>
<p>                    AST节点和令牌不是一对一的对应关系。</p>
<p>                    输入的顺序也不同</p>
<p>                    节点和令牌的类型也不同</p>
<p>                模型组成不同</p>
<p>                    对AST级的代码完成设计了path2root编码器来捕获树的分层结构信息，由于令牌序列中没有明确的树结构，因此不能应用于令牌级完成。</p>
<p>        很难将我们的模型应用到TDD场景中，模型无法预测在任何地方没有存在的一段代码</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>    本文构建了一个可以在令牌级和AST级执行代码完成的统一框架，这允许我们执行一系列设计决策，这些决策可以帮助我们在完成系统的期望属性之间进行良好的权衡。此外，我们还探讨了不同的模型设置或技术是否对不同的输入格式（令牌和AST）执行相同的操作，从而可以为以后的代码完成研究提供参考</p>
<p>    在本文中，我们将MTL应用于代码完成，以联合预测下一个代码元素的类型和值，并在统计上显著和实质性地改进最新技术。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>    提出了一个统一的基于多任务学习的框架，用于AST级和令牌级代码完成</p>
<p>    设计了一个代码元素编码器，将每个代码元素编码为分布式矢量表示。</p>
<p>    为了捕获程序中的长期依赖性，我们构建了一个基于Transformer XL网络的上下文代码编码器，将上下文代码编码为分布式矢量表示。</p>
<p>    为了显式地对程序的分层信息进行建模，我们提出了一种新的Path2root编码器来编码从预测节点到根节点的AST路径。</p>
<p>    为了利用代码元素的类型和值之间的关系，我们应用MTL框架联合预测下一个代码元素的种类和值，从而实现这两个任务之间的知识共享。</p>
<p>    实验结果表明，所提出的模型在AST级和令牌级代码完成方面都比以前的最新模型取得了更好的结果。</p>
<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/a1234556667/article/details/126447044">https://blog.csdn.net/a1234556667/article/details/126447044</a>代码中的长期依赖</p>
<p><a target="_blank" rel="noopener" href="https://111hunter.github.io/2020-08-23-ast/">https://111hunter.github.io/2020-08-23-ast/</a>什么是AST</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27201427">https://zhuanlan.zhihu.com/p/27201427</a>统计语言模型</p>
<p><a target="_blank" rel="noopener" href="https://www.anquanke.com/post/id/221892">https://www.anquanke.com/post/id/221892</a>基于抽象语法树和深度学习的高效漏洞检测方法中前面AST和神经网络相关的内容</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/265108616">https://zhuanlan.zhihu.com/p/265108616</a>注意机制与自注意机制</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580130/article/details/107676664">https://blog.csdn.net/weixin_43580130&#x2F;article&#x2F;details&#x2F;107676664</a>前馈、全连接神经网络</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/38002635">https://www.zhihu.com/question/38002635</a>embedding概念</p>
<p><a target="_blank" rel="noopener" href="https://xcgfth.github.io/2019/05/06/OOV%E9%97%AE%E9%A2%98%E5%92%8CBPE%E7%AE%97%E6%B3%95/#more">https://xcgfth.github.io/2019/05/06/OOV%E9%97%AE%E9%A2%98%E5%92%8CBPE%E7%AE%97%E6%B3%95/#more</a>Oov问题和BPE</p>
<p><a target="_blank" rel="noopener" href="https://teahouse.fifty-five.com/zh-hans/glossary/overfitting/">https://teahouse.fifty-five.com/zh-hans/glossary/overfitting/</a>过度拟合的问题</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32829048">https://zhuanlan.zhihu.com/p/32829048</a>N-gram模型介绍</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>A_Niucw
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2022/11/09/AST%E7%BA%A7%E5%92%8Ctoken%E7%BA%A7%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%88%90%E7%9A%84%E7%BB%9F%E4%B8%80%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/" title="AST级和token级代码完成的统一多任务学习模型">http://example.com/2022/11/09/AST级和token级代码完成的统一多任务学习模型/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/10/19/Kimsuky%E6%A0%B7%E6%9C%AC%E6%94%BB%E5%87%BB%E6%89%8B%E6%AE%B5%E5%88%86%E6%9E%90/" rel="prev" title="Kimsuky样本攻击手段分析">
      <i class="fa fa-chevron-left"></i> Kimsuky样本攻击手段分析
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#AST%E7%BA%A7%E5%92%8Ctoken%E7%BA%A7%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%88%90%E7%9A%84%E7%BB%9F%E4%B8%80%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.</span> <span class="nav-text">AST级和token级代码完成的统一多任务学习模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract%EF%BC%9A"><span class="nav-number">1.1.</span> <span class="nav-text">Abstract：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Motivating-Example"><span class="nav-number">1.3.</span> <span class="nav-text">Motivating Example</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Background"><span class="nav-number">1.4.</span> <span class="nav-text">Background</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Proposed-Model%EF%BC%88UMTLM%E6%A8%A1%E5%9E%8B%EF%BC%89"><span class="nav-number">1.5.</span> <span class="nav-text">Proposed Model（UMTLM模型）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Experiment-and-Analysis"><span class="nav-number">1.6.</span> <span class="nav-text">Experiment and Analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Discussion"><span class="nav-number">1.7.</span> <span class="nav-text">Discussion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Related-Work"><span class="nav-number">1.8.</span> <span class="nav-text">Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">1.9.</span> <span class="nav-text">Conclusion</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">1.9.1.</span> <span class="nav-text">参考链接</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="A_Niucw"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">A_Niucw</p>
  <div class="site-description" itemprop="description">静下来搞技术</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span>
      </div>
  </nav>
</div>



      </div>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1962385461&auto=1&height=66"></iframe>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">A_Niucw</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
